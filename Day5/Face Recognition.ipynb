{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "## Detecting objects using Haar Cascade Classifier\n",
    "\n",
    "\n",
    "### What are Haar Cascades?\n",
    "Haar Cascade classifiers are an effective way for object detection. This method was proposed by Paul Viola and Michael Jones in their paper Rapid Object Detection using a Boosted Cascade of Simple Features. <br>\n",
    "Haar Cascade is a machine learning-based approach where a lot of positive and negative images are used to train the classifier.\n",
    "\n",
    "    Positive images – These images contain the images which we want our classifier to identify.\n",
    "    Negative Images – Images of everything else, which do not contain the object we want to detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ourClassifier*.**detectMultiScale**(input image, **Scale Factor** , **Min Neighbors**)\n",
    "\n",
    "- **Scale Factor**\n",
    "Specifies how much we reduce the image size each time we scale. E.g. in face detection we typically use 1.3. This means we reduce the image by 30% each time it’s scaled. Smaller values, like 1.05 will take longer to compute, but will increase the rate of detection.\n",
    "\n",
    "- **Min Neighbors**\n",
    "Specifies the number of neighbors each potential window should have in order to consider it a positive detection. Typically set between 3-6. \n",
    "It acts as sensitivity setting, low values will sometimes detect multiples faces over a single face. High values will ensure less false positives, but you may miss some faces.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>detectMultiScale </b> return 4 values\n",
    "\n",
    "harcascade only accept greyscale inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOLO \n",
    "yolo is a machine learning algo for object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the cascade\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "# Read the input image\n",
    "img = cv2.imread('group-photo.png')\n",
    "cv2.imshow('Image',img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# Detect faces\n",
    "faces = face_cascade.detectMultiScale(gray, 1.5, 5)\n",
    "# Draw rectangle around the faces\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "# Display the output\n",
    "cv2.imshow('Faces',img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eye Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    " \n",
    "eye_classifier = cv2.CascadeClassifier('haarcascade_eye_tree_eyeglasses.xml')\n",
    "img = cv2.imread('eye.jpg')\n",
    "cv2.imshow('Face',img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "eyes = eye_classifier.detectMultiScale(gray, 1.1,5)\n",
    "for (ex,ey,ew,eh) in eyes:\n",
    "    cv2.rectangle(img,(ex,ey),(ex+ew,ey+eh),(255,255,0),2)\n",
    "cv2.imshow('Eyes',img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    " \n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eye_classifier = cv2.CascadeClassifier('haarcascade_eye_tree_eyeglasses.xml')\n",
    " \n",
    "img = cv2.imread('self.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "faces = face_classifier.detectMultiScale(gray, 1.1, 5)\n",
    "\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(127,0,255),2)\n",
    "    cv2.imshow('Face',img)\n",
    "\n",
    "\n",
    "    eyes = eye_classifier.detectMultiScale(gray, 1.1,5)\n",
    "    for (ex,ey,ew,eh) in eyes:\n",
    "        cv2.rectangle(img,(ex,ey),(ex+ew,ey+eh),(255,255,0),2)\n",
    "cv2.imshow('Multi',img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO (You Only Look Once) real-time object detection algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " YOLO trained on the COCO dataset.\n",
    "\n",
    "The COCO dataset consists of 80 labels, including, but not limited to:\n",
    "\n",
    "    People\n",
    "    Bicycles\n",
    "    Cars and trucks\n",
    "    Airplanes\n",
    "    Stop signs and fire hydrants\n",
    "    Animals, including cats, dogs, birds, horses, cows, and sheep, to name a few\n",
    "    Kitchen and dining objects, such as wine glasses, cups, forks, knives, spoons, etc.\n",
    "    …and much more!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# python yolo.py --image images/baggage_claim.jpg --yolo yolo-coco\n",
    "\n",
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-i\", \"--image\", required=True,\n",
    "# \thelp=\"path to input image\")\n",
    "# ap.add_argument(\"-y\", \"--yolo\", required=True,\n",
    "# \thelp=\"base path to YOLO directory\")\n",
    "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
    "\thelp=\"minimum probability to filter weak detections\")\n",
    "ap.add_argument(\"-t\", \"--threshold\", type=float, default=0.3,\n",
    "\thelp=\"threshold when applyong non-maxima suppression\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# load the COCO class labels our YOLO model was trained on\n",
    "labelsPath = \"/content/gdrive/My Drive/GSSummit/yolo/coco.names\"\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "# initialize a list of colors to represent each possible class label\n",
    "np.random.seed(42)\n",
    "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\n",
    "\tdtype=\"uint8\")\n",
    "\n",
    "# derive the paths to the YOLO weights and model configuration\n",
    "weightsPath = \"/content/gdrive/My Drive/GSSummit/yolo/yolov3.weights\"\n",
    "configPath = \"/content/gdrive/My Drive/GSSummit/yolo/yolov3.cfg\"\n",
    "\n",
    "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "print(\"[INFO] loading YOLO from disk...\")\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
    "\n",
    "# load our input image and grab its spatial dimensions\n",
    "image = cv2.imread(args[\"image\"])\n",
    "(H, W) = image.shape[:2]\n",
    "\n",
    "# determine only the *output* layer names that we need from YOLO\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# construct a blob from the input image and then perform a forward\n",
    "# pass of the YOLO object detector, giving us our bounding boxes and\n",
    "# associated probabilities\n",
    "blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
    "\tswapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "start = time.time()\n",
    "layerOutputs = net.forward(ln)\n",
    "end = time.time()\n",
    "\n",
    "# show timing information on YOLO\n",
    "print(\"[INFO] YOLO took {:.6f} seconds\".format(end - start))\n",
    "\n",
    "# initialize our lists of detected bounding boxes, confidences, and\n",
    "# class IDs, respectively\n",
    "boxes = []\n",
    "confidences = []\n",
    "classIDs = []\n",
    "\n",
    "# loop over each of the layer outputs\n",
    "for output in layerOutputs:\n",
    "\t# loop over each of the detections\n",
    "\tfor detection in output:\n",
    "\t\t# extract the class ID and confidence (i.e., probability) of\n",
    "\t\t# the current object detection\n",
    "\t\tscores = detection[5:]\n",
    "\t\tclassID = np.argmax(scores)\n",
    "\t\tconfidence = scores[classID]\n",
    "\n",
    "\t\t# filter out weak predictions by ensuring the detected\n",
    "\t\t# probability is greater than the minimum probability\n",
    "\t\tif confidence > args[\"confidence\"]:\n",
    "\t\t\t# scale the bounding box coordinates back relative to the\n",
    "\t\t\t# size of the image, keeping in mind that YOLO actually\n",
    "\t\t\t# returns the center (x, y)-coordinates of the bounding\n",
    "\t\t\t# box followed by the boxes' width and height\n",
    "\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
    "\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t# use the center (x, y)-coordinates to derive the top and\n",
    "\t\t\t# and left corner of the bounding box\n",
    "\t\t\tx = int(centerX - (width / 2))\n",
    "\t\t\ty = int(centerY - (height / 2))\n",
    "\n",
    "\t\t\t# update our list of bounding box coordinates, confidences,\n",
    "\t\t\t# and class IDs\n",
    "\t\t\tboxes.append([x, y, int(width), int(height)])\n",
    "\t\t\tconfidences.append(float(confidence))\n",
    "\t\t\tclassIDs.append(classID)\n",
    "\n",
    "# apply non-maxima suppression to suppress weak, overlapping bounding\n",
    "# boxes\n",
    "idxs = cv2.dnn.NMSBoxes(boxes, confidences, args[\"confidence\"],\n",
    "\targs[\"threshold\"])\n",
    "\n",
    "# ensure at least one detection exists\n",
    "if len(idxs) > 0:\n",
    "\t# loop over the indexes we are keeping\n",
    "\tfor i in idxs.flatten():\n",
    "\t\t# extract the bounding box coordinates\n",
    "\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
    "\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "\t\t# draw a bounding box rectangle and label on the image\n",
    "\t\tcolor = [int(c) for c in COLORS[classIDs[i]]]\n",
    "\t\tcv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "\t\ttext = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
    "\t\tcv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "\t\t\t0.5, color, 2)\n",
    "\n",
    "# show the output image\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
